{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546b6c6d-f949-4387-9c41-6989223911f8",
   "metadata": {},
   "source": [
    "# Initializing weights with LoftQ by replacing LoRA weights in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041ecb4-6957-467e-8f3e-d4a12c674e9f",
   "metadata": {},
   "source": [
    "This notebook shows how to apply [LoftQ](https://arxiv.org/abs/2310.08659) initialization on our QLoRA model.\n",
    "\n",
    "In short, the idea behind LoftQ is the following. When we use QLoRA, i.e. we quantize the base model with bitsandbytes to save memory, and then train LoRA weights on top of this base model, we expect a certain performance gap. This is partly due to the fact that quantization is onyl an approximation of the \"real\" weights and thus introduces a quantization error. By default, LoRA weights are initialized such that they are a no-op at the start of the training. However, we can instead initialize them so that they minimize the quantization error. This is the idea behind LoftQ.\n",
    "\n",
    "Note that this only influences the initialization of the model. Everything that follows stays the same as always."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5420f-de32-42fa-8792-247f60e3647d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c69b7c-c922-405f-aae1-ccc4f6911155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22be0432-8798-44a2-9014-d929525e3059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esthersong/miniconda3/envs/loftq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f087ce0f-71b4-45ec-b2f9-197677bbc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, replace_lora_weights_loftq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdf18e-4ac4-409e-8475-88147cf85067",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af14bd0a-597e-446c-800b-619fc0599ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(x, y):\n",
    "    return (x - y).abs().mean()\n",
    "\n",
    "\n",
    "def get_mse(x, y):\n",
    "    return torch.pow(x - y, 2).mean()\n",
    "\n",
    "\n",
    "def error_report(x, y):\n",
    "    mae = get_mae(x, y)\n",
    "    mse = get_mse(x, y)\n",
    "    print(\n",
    "        f\"Mean absolute error: {mae:>8.5f}\\n\"\n",
    "        f\"Mean squared error:  {mse:>8.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc01a5f-7ee8-400f-8e80-3f2b7df29882",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc447d9-2f4f-4d0f-afdb-1cf5c4237321",
   "metadata": {},
   "source": [
    "First, let's load a base model and calculate some logits. These logits are the baseline, i.e. we try to match their values as best as possible. We only need these logits for demonstration purposes. In practice, it is not necessary to load the non-quantized weights to apply LoftQ initialization.\n",
    "\n",
    "**Note**: We have to choose a model with a `model.safetensors` file. As PyTorch checkpoints (pickle) cannot be loaded lazily, we have to use [safetensors](https://huggingface.co/docs/safetensors/index). If those don't exist for your model, save the pretrained model as a safetensors file using `safe_pretrained` and pass the model path to `replace_lora_weights_loftq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb29074-d180-4fdc-8a47-27d2b9857264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bigscience/bloomz-560m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ddd6a2-04dd-42ec-9f48-100a3946ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b27db-51cc-41da-a21d-049ff747a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "## baseline: Full Precision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51548b6a-945c-4797-b02a-0e3fc77d1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "Readability counts.\n",
    "Special cases aren't special enough to break the rules.\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "Namespaces are one honking great idea -- let's do more of those!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce72d923-5283-48ba-96ef-7f859309ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(s.splitlines(), return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe54cb-76ef-4981-ba25-3e544d264c62",
   "metadata": {},
   "source": [
    "Our baseline logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bebcaa-3a05-4621-9a03-e25de72fa27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_base = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16025063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[278.0033, 283.7906, 297.8988,  ..., 161.5043, 161.5043, 161.4948],\n",
       "         [278.0033, 283.7906, 297.8988,  ..., 161.5043, 161.5043, 161.4948],\n",
       "         [278.0033, 283.7906, 297.8988,  ..., 161.5043, 161.5043, 161.4948],\n",
       "         ...,\n",
       "         [401.2244, 399.7368, 418.2438,  ..., 207.4830, 207.4830, 207.4720],\n",
       "         [399.9124, 404.4676, 429.3989,  ..., 206.6692, 206.6694, 206.6578],\n",
       "         [393.6312, 398.5472, 420.0659,  ..., 207.6603, 207.6605, 207.6490]],\n",
       "\n",
       "        [[318.6148, 324.2510, 338.7469,  ..., 179.2935, 179.2938, 179.2829],\n",
       "         [318.6148, 324.2510, 338.7469,  ..., 179.2935, 179.2938, 179.2829],\n",
       "         [318.6148, 324.2510, 338.7469,  ..., 179.2935, 179.2938, 179.2829],\n",
       "         ...,\n",
       "         [397.1699, 397.2800, 416.4278,  ..., 203.6698, 203.6704, 203.6588],\n",
       "         [399.3676, 403.9751, 427.0152,  ..., 205.7427, 205.7434, 205.7319],\n",
       "         [400.2210, 402.1478, 423.6199,  ..., 206.3044, 206.3050, 206.2930]],\n",
       "\n",
       "        [[265.3966, 271.5363, 285.0104,  ..., 155.1103, 155.1102, 155.1010],\n",
       "         [265.3966, 271.5363, 285.0104,  ..., 155.1103, 155.1102, 155.1010],\n",
       "         [265.3966, 271.5363, 285.0104,  ..., 155.1103, 155.1102, 155.1010],\n",
       "         ...,\n",
       "         [402.9583, 402.1131, 420.0234,  ..., 208.4720, 208.4721, 208.4610],\n",
       "         [401.0695, 404.1194, 427.9299,  ..., 206.8629, 206.8634, 206.8517],\n",
       "         [400.5481, 403.5322, 423.3956,  ..., 208.7852, 208.7856, 208.7736]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[393.8611, 398.4102, 412.7114,  ..., 209.2024, 209.2027, 209.1914],\n",
       "         [393.8611, 398.4102, 412.7114,  ..., 209.2024, 209.2027, 209.1914],\n",
       "         [393.8611, 398.4102, 412.7114,  ..., 209.2024, 209.2027, 209.1914],\n",
       "         ...,\n",
       "         [394.0483, 397.7760, 418.8911,  ..., 204.2415, 204.2421, 204.2299],\n",
       "         [402.3118, 405.2349, 434.2230,  ..., 205.7666, 205.7671, 205.7552],\n",
       "         [400.9548, 403.9235, 426.7352,  ..., 207.2128, 207.2128, 207.2009]],\n",
       "\n",
       "        [[398.2768, 403.8636, 419.8150,  ..., 205.5676, 205.5678, 205.5564],\n",
       "         [398.2768, 403.8636, 419.8150,  ..., 205.5676, 205.5678, 205.5564],\n",
       "         [285.5006, 287.3928, 300.0596,  ..., 173.8300, 173.8299, 173.8220],\n",
       "         ...,\n",
       "         [397.1454, 398.6783, 420.5297,  ..., 204.6029, 204.6034, 204.5914],\n",
       "         [402.5493, 405.7154, 432.7310,  ..., 206.2852, 206.2857, 206.2739],\n",
       "         [399.5277, 403.0315, 426.2778,  ..., 206.7712, 206.7712, 206.7594]],\n",
       "\n",
       "        [[402.7061, 404.5195, 423.4741,  ..., 207.2598, 207.2601, 207.2487],\n",
       "         [402.7061, 404.5195, 423.4741,  ..., 207.2598, 207.2601, 207.2487],\n",
       "         [316.6784, 319.9539, 335.3265,  ..., 189.5296, 189.5295, 189.5198],\n",
       "         ...,\n",
       "         [403.2892, 400.6911, 424.9332,  ..., 205.2396, 205.2397, 205.2284],\n",
       "         [401.6398, 400.5832, 424.0989,  ..., 204.6241, 204.6243, 204.6130],\n",
       "         [401.8586, 405.0751, 427.5526,  ..., 206.8238, 206.8241, 206.8120]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c9001-8ade-422d-92f8-bcafa50917c7",
   "metadata": {},
   "source": [
    "## Normal LoRA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024390b-736a-4b21-848b-aa4f30951d51",
   "metadata": {},
   "source": [
    "Now we load the model quantized with bitsandbytes. For now, only 4bit is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d1912a-646e-42d2-8292-6702b77d1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1218717-4db4-48ce-978d-c05dc190fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "\n",
    "## PTQ model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4e4c5-3932-4d9a-9457-41a05f24d556",
   "metadata": {},
   "source": [
    "Next we create a LoRA model using PEFT and compute the logits of that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4741bce0-cd2b-4f05-a50c-4f9e56b43e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(task_type=\"CAUSAL_LM\", target_modules=\"all-linear\")\n",
    "\n",
    "## QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf55cc48-b55d-4806-b6ab-e9b8035ed526",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f11e25-4a1e-485b-be4c-65aec62ac207",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits_lora \u001b[38;5;241m=\u001b[39m \u001b[43mpeft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:986\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    984\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 986\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1000\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:632\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    629\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n\u001b[1;32m    635\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loftq/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "logits_lora = peft_model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0cde7-0b9f-4305-ac0e-e3a6d2cfa401",
   "metadata": {},
   "source": [
    "Let's check the influence of the quantization error on our logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f404c0d-f428-4923-9122-7b830410f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  3.61113\n",
      "Mean squared error:  36.53259\n"
     ]
    }
   ],
   "source": [
    "error_report(logits_base, logits_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c437e1-4fae-4a2f-9c42-ada6bedb9a4d",
   "metadata": {},
   "source": [
    "## LoftQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af05376-c8b0-48ec-8d80-7d7f4d32bbd7",
   "metadata": {},
   "source": [
    "Next, let's use LoftQ initialization and see if it helps reduce the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890e6108-3f02-469c-9e7d-f2144448227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_lora_weights_loftq(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b452db0e-a510-42d3-bef5-f567186e26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_loftq = peft_model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "456dc564-f268-4cf3-9d59-a6942d3733ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  3.24111\n",
      "Mean squared error:  31.13725\n"
     ]
    }
   ],
   "source": [
    "error_report(logits_base, logits_loftq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf9e0f-3f78-426c-be59-77c6481674ec",
   "metadata": {},
   "source": [
    "We can see that LoftQ initialization helped a little bit, but the difference is not huge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd344f2-249c-4fe9-8357-7fe3bcd1e82f",
   "metadata": {},
   "source": [
    "## LoftQ with callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd7dd5-88b3-40b8-95c2-3f3895d8093d",
   "metadata": {},
   "source": [
    "To help with this, let's write a small callback function and pass it to `replace_lora_weights_loftq`. What this function does is that each time one weight is being replaced with LoftQ-initialized weights, we perform a test if the quantization error is actually reduced. If it it is not, we roll back the replacement. This way, we keep only those replacements that improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f882802-22b7-4969-919e-120b1f2893d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# Since PEFT has modified the base model, we should reload it\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6438363-b66e-4507-8667-5a6df379a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# QLoRA (기본본)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b93d082-0fcb-4b20-982e-c1aaf0c71d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_mse = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22eb18d-b06e-47fe-91ba-ff34cbf62f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_callback(model, module_name):\n",
    "    \"\"\"Callable to replace weights with LoFTQ if the mse is lower than the current best one.\"\"\"\n",
    "    global current_mse\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    mse = get_mse(logits_base, logits)\n",
    "    if mse < current_mse:\n",
    "        current_mse = mse\n",
    "        print(f\"MSE improved for module {module_name}\")\n",
    "        return True\n",
    "    print(f\"MSE did not improve for module {module_name}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee90d1-e15a-4740-a39d-ebf9e7adb79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "replace_lora_weights_loftq(peft_model, callback=my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e31adc81-a090-49b2-90f6-9906743c76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_loftq_callback = peft_model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c640092-1f26-48be-bea4-487511205440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  1.79576\n",
      "Mean squared error:   8.47075\n"
     ]
    }
   ],
   "source": [
    "error_report(logits_base, logits_loftq_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896857e-3d87-44a9-887f-90c765bc8d91",
   "metadata": {},
   "source": [
    "We can see that applying LoftQ with the help of the callback reduced the error quite significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf86cf-4fb4-455d-ab07-892591564303",
   "metadata": {},
   "source": [
    "## Applying LoftQ multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70836a75-5c6d-4b7b-9175-f395aef8383b",
   "metadata": {},
   "source": [
    "It is possible to run `replace_lora_weights_loftq` multiple times on the same model when using the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e5ee38c-007c-4c75-9248-005d94b19445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE did not improve for module transformer.h.0.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.0.self_attention.dense\n",
      "MSE did not improve for module transformer.h.0.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.0.mlp.dense_4h_to_h\n",
      "MSE improved for module transformer.h.1.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.1.self_attention.dense\n",
      "MSE did not improve for module transformer.h.1.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.1.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.2.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.2.self_attention.dense\n",
      "MSE did not improve for module transformer.h.2.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.2.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.3.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.3.self_attention.dense\n",
      "MSE did not improve for module transformer.h.3.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.3.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.4.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.4.self_attention.dense\n",
      "MSE did not improve for module transformer.h.4.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.4.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.5.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.5.self_attention.dense\n",
      "MSE did not improve for module transformer.h.5.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.5.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.6.self_attention.query_key_value\n",
      "MSE improved for module transformer.h.6.self_attention.dense\n",
      "MSE did not improve for module transformer.h.6.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.6.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.7.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.7.self_attention.dense\n",
      "MSE did not improve for module transformer.h.7.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.7.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.8.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.8.self_attention.dense\n",
      "MSE did not improve for module transformer.h.8.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.8.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.9.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.9.self_attention.dense\n",
      "MSE did not improve for module transformer.h.9.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.9.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.10.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.10.self_attention.dense\n",
      "MSE improved for module transformer.h.10.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.10.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.11.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.11.self_attention.dense\n",
      "MSE did not improve for module transformer.h.11.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.11.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.12.self_attention.query_key_value\n",
      "MSE improved for module transformer.h.12.self_attention.dense\n",
      "MSE did not improve for module transformer.h.12.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.12.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.13.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.13.self_attention.dense\n",
      "MSE did not improve for module transformer.h.13.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.13.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.14.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.14.self_attention.dense\n",
      "MSE did not improve for module transformer.h.14.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.14.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.15.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.15.self_attention.dense\n",
      "MSE did not improve for module transformer.h.15.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.15.mlp.dense_4h_to_h\n",
      "MSE improved for module transformer.h.16.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.16.self_attention.dense\n",
      "MSE did not improve for module transformer.h.16.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.16.mlp.dense_4h_to_h\n",
      "MSE improved for module transformer.h.17.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.17.self_attention.dense\n",
      "MSE did not improve for module transformer.h.17.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.17.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.18.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.18.self_attention.dense\n",
      "MSE did not improve for module transformer.h.18.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.18.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.19.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.19.self_attention.dense\n",
      "MSE did not improve for module transformer.h.19.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.19.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.20.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.20.self_attention.dense\n",
      "MSE did not improve for module transformer.h.20.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.20.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.21.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.21.self_attention.dense\n",
      "MSE did not improve for module transformer.h.21.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.21.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.22.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.22.self_attention.dense\n",
      "MSE did not improve for module transformer.h.22.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.22.mlp.dense_4h_to_h\n",
      "MSE did not improve for module transformer.h.23.self_attention.query_key_value\n",
      "MSE did not improve for module transformer.h.23.self_attention.dense\n",
      "MSE did not improve for module transformer.h.23.mlp.dense_h_to_4h\n",
      "MSE did not improve for module transformer.h.23.mlp.dense_4h_to_h\n"
     ]
    }
   ],
   "source": [
    "replace_lora_weights_loftq(peft_model, callback=my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2abe2702-9510-4814-b5f2-63140a102c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_loftq_callback_twice = peft_model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e908de14-01f9-4fdc-91b5-61118a3ce6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  1.76357\n",
      "Mean squared error:   8.33938\n"
     ]
    }
   ],
   "source": [
    "error_report(logits_base, logits_loftq_callback_twice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b09fe-d369-4444-b6e2-cd514e775637",
   "metadata": {},
   "source": [
    "There are further gains, but they are not very big."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loftq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
